# DA5401 Data Challenge 2025: Metric Score Prediction

**Author:** Manish Nayak  
**Roll No. :** CE22B069 

## üìå Project Overview

This repository contains the solution for the DA5401 Data Challenge. The objective is to predict a deterministic **'metric score' (0 to 10)** for a given prompt and response pair, based on a specific evaluation metric.

The primary challenge addressed in this solution is the **severe class imbalance** in the training data, where high scores (9.0‚Äì10.0) dominate, and low scores (0.0‚Äì6.0) are extremely rare.

## üõ†Ô∏è Installation & Requirements

The project requires a Python environment with GPU support (CUDA).

### Key Dependencies
*   **Python:** 3.11+
*   **PyTorch:** (Deep Learning Framework)
*   **SentenceTransformers:** (Text Embedding)
*   **Optuna:** (Hyperparameter Optimization)
*   **XGBoost:** (Gradient Boosting)
*   **Imbalanced-learn:** (SMOTE-NC implementation)
*   **Scikit-learn:** (Data splitting and metrics)

### Install Commands
```bash
pip install torch sentence-transformers optuna xgboost scikit-learn==1.1.3 imbalanced-learn==0.10.1 pandas numpy
```

## üìÇ Dataset & Preprocessing

### Input Data
*   `train_data.json`: Contains `system_prompt`, `user_prompt`, `response`, `metric_name`, and `score`.
*   `test_data.json`: Same structure, excluding `score`.
*   `metric_name_embeddings.npy`: Pre-computed embeddings for metric names.

### Feature Engineering
All textual data is converted into 768-dimensional vectors using the **`google/embeddinggemma-300m`** model.
1.  **Prompt Embedding:** Concatenation of System Prompt + User Prompt.
2.  **Response Embedding:** The model response text.
3.  **Metric Embedding:** Context vector for the specific metric being evaluated.

### Data Cleaning
Rare fractional scores were consolidated to standardize the target variable (e.g., mapping 9.5 ‚Üí 10.0, 4.0 ‚Üí 3.0).

---

## üß† Methodology

To address the class imbalance and regression complexity, a **two-model ensemble strategy** was employed.

### Model 1: Hierarchical Attention Network (Specialist)
*   **Goal:** Deep semantic understanding to identify low-scoring samples.
*   **Architecture:**
    *   **Layer 1:** Cross-Attention (Prompt ‚Üî Response) to generate a context-aware prompt.
    *   **Layer 2:** Cross-Attention (Metric ‚Üî Context-Aware Prompt) to align with the evaluation criteria.
    *   **Head:** Regression MLP.
*   **Optimization (Optuna):** Used to tune hyperparameters (LR, Dropout, Heads) and **custom loss weights**.
*   **Loss Function:** Weighted MSE. Weights were dynamically tuned to penalize errors on rare classes (scores < 6.0) significantly more (Weight ‚âà 1000x) than common classes.

### Model 2: VQ-VAE + SMOTE-NC + XGBoost (Generalist)
*   **Goal:** Feature engineering approach to synthetically balance the data.
*   **Pipeline:**
    1.  **Discretization (VQ-VAE):** Three Vector Quantized Variational Autoencoders compressed the continuous embeddings into a discrete codebook (Size: 256).
    2.  **Oversampling (SMOTE-NC):** Applied Synthetic Minority Over-sampling Technique for Nominal/Continuous features on the discrete codes. Rare scores were oversampled by factors up to 40x.
    3.  **Regression (XGBoost):** A gradient boosting regressor trained on the balanced, discrete dataset.

### üèÜ Ensemble Strategy
The final submission is generated by a simple **average** of the predictions from Model 1 and Model 2. This combines the specialized sensitivity of the Attention Network with the regularized, balanced view of the XGBoost pipeline.

---

## üöÄ Usage Instructions

The solution is contained within the Jupyter Notebook `da-hackathon-clean-code-file.ipynb`.

### Step 1: Authentication & Setup
Ensure you have access to HuggingFace (for the Gemma model) and Kaggle (if running in that environment).

### Step 2: Train Model 1 (Attention)
Run the cells in **Section 2** of the notebook.
*   This will perform an Optuna search (150 trials) to find the best weights.
*   It saves the best model as `best_attn_model_fully_tuned.pth`.
*   Generates `submission_attn_regression_fully_tuned.csv`.

### Step 3: Train Model 2 (VQ-VAE + XGB)
Run the cells in **Section 3** of the notebook.
*   Trains 3 VQ-VAEs to discretize features.
*   Applies SMOTE-NC to balance the dataset.
*   Trains XGBoost on the resampled data.
*   Generates `submission_vqvae_xgb_tiered_smotenc.csv`.

### Step 4: Generate Final Submission
Manually average the scores from the two generated CSV files to produce the final submission file.

```python
import pandas as pd

# Load predictions
df1 = pd.read_csv('submission_attn_regression_fully_tuned.csv')
df2 = pd.read_csv('submission_vqvae_xgb_tiered_smotenc.csv')

# Average
final_df = df1.copy()
final_df['score'] = (df1['score'] + df2['score']) / 2

# Save
final_df.to_csv('final_ensemble_submission.csv', index=False)
```

---

## üìú License
This project is created for the DA5401 2025 Data Challenge.
